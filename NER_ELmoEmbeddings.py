# -*- coding: utf-8 -*-
"""NER_BiDirectional-LSTM

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14CZ2prIY9Ott_6PVOxr7s4a-Xb79WcKV
"""

import pandas as pd
import numpy as np 
import matplotlib.pyplot as plt 
plt.style.use('ggplot')

# pip install "tensorflow==1.15.0"
!pip show tensorflow

pip install tensorflow-hub

data = pd.read_csv('/content/drive/MyDrive/Data/snake_breed_resnet50.hf5/ner_dataset.csv', encoding='latin1')
data = data.drop(['POS'], axis=1)
data = data.fillna(method='ffill')
data.shape

#remove duplicate words using set
words = set(list(data['Word'].values))
words.add('PADword')
n_words = len(words)
n_words
#there are 35179 different words in this dataset

tags = list(set(data['Tag'].values))
n_tags = len(tags)
n_tags
#there are 17 kind of different tags

class SentenceGetter(object):
  def __init__(self, data):
    self.data = data
    self.n_sent = 1
    self.empty = False
    agg_func = lambda x: [(w,t) for w,t  in zip(x['Word'].values.tolist(), x['Tag'].values.tolist())]
    self.grouped = self.data.groupby('Sentence #').apply(agg_func)
    self.sentences = [s for s in self.grouped]

getter = SentenceGetter(data)
sentences = getter.sentences

#largest sentences
max_len = max(len(sen) for sen in sentences)
print(max_len)

plt.hist([len(sen) for sen in sentences], bins=50)
plt.show()
#so the longest sentence has 140 words in it and we can see that almost all of the sentences have less than 60 words in them.

X = [[w[0] for w in s] for s in sentences]
max_len = max(len(sent) for sent in sentences)  
X[0]

max_len = 50
X = [[w[0] for w in s] for s in sentences]
new_X = []
for seq in X:
  new_seq = []
  for i in range(max_len):
    try:
      new_seq.append(seq[i])
    except:
      new_seq.append('PADword')
  new_X.append(new_seq)  
X = new_X

from tensorflow.keras.preprocessing.sequence import pad_sequences

tags2index = {t:i for i,t in enumerate(tags)}
y = [[tags2index[w[1]] for w in s] for s in sentences]
y = pad_sequences(maxlen=max_len, padding='post', sequences=y, value=tags2index['O'])
y[1]

from sklearn.model_selection import train_test_split
import tensorflow as tf
import tensorflow_hub as hub
import numpy as np
from tensorflow.keras import backend as k

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1, random_state=2018)

#Initialize tensorflow session
sess = tf.Session()
k.set_session(sess)
elmo_model = hub.Module("https://tfhub.dev/google/elmo/2", trainable=True)
sess.run(tf.global_variables_initializer())
sess.run(tf.tables_initializer())

batch_size = 32

def ElmoEmbedding(x):
    return elmo_model(inputs={"tokens": tf.squeeze(tf.cast(x,    tf.string)),"sequence_len": tf.constant(batch_size*[max_len])
                     },
                      signature="tokens",
                      as_dict=True)["elmo"]

from tensorflow.keras.layers import Dense, Flatten, Embedding, LSTM, TimeDistributed, Dropout, Bidirectional, Lambda
from tensorflow.keras.models import Model
from tensorflow.keras.layers import concatenate
from tensorflow.keras import Input



input = Input(shape=(max_len,), dtype=tf.string)
embeddings = Lambda(ElmoEmbedding, output_shape=(max_len, 1024))(input)
x = Bidirectional(LSTM(units=512, return_sequences=True, recurrent_dropout=0.2, dropout=0.2))(embeddings)
x_rnn = Bidirectional(LSTM(units=512, return_sequences=True, recurrent_dropout=0.2, dropout=0.2))(x)
x = concatenate([x, x_rnn])
out = TimeDistributed(Dense(n_tags, activation='softmax'))(x)

model = Model(input, out)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

X_tr, X_val = X_train[:1213*batch_size], X_train[-135*batch_size:]
y_tr, y_val = y_train[:1213*batch_size], y_train[-135*batch_size:]
y_tr = y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)
y_val = y_val.reshape(y_val.shape[0], y_val.shape[1], 1)

m = np.array(X_tr)
print(m.shape)
n = np.array(X_val)
print(n.shape)

history = model.fit(np.array(X_tr), y_tr, validation_data=(np.array(X_val), y_val),
                    batch_size=batch_size, epochs=5, verbose=1)

X_te = X_test[:149*batch_size]
test_pred = model.predict(np.array(X_te), verbose=1)

pip install seqeval

from seqeval.metrics import classification_report

idx2tags = {i:w for w, i in tags2index.items()}

def pred2label(pred):
  out = []
  for pred_i in pred:
    out_i = []
    for p in pred_i:
      p_i = np.argmax(p)
      out_i.append(idx2tags[p_i].replace('PADword','O'))
    out.append(out_i)
  return out

def test2label(pred):
  out = []
  for pred_i in pred:
    out_i = []
    for p in pred_i:
      out_i.append(idx2tags[p].replace('PADword','O'))
    out.append(out_i)
  return out

test_preds = pred2label(test_pred)
test_labels = test2label(y_test[:149*32])

print(classification_report(test_labels, test_preds))